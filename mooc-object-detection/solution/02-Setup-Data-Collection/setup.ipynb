{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "<br />\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/dtlogo.png\" alt=\"Logo\" width=\"111\" height=\"100\">\n",
    "\n",
    "  <h1 align=\"center\">Object detection</h1>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "### Object Detection\n",
    "\n",
    "Machine-learned object detection models can be extremely useful. They are faster and often more reliable than most traditional computer vision models. Aditionnally, we can use pretrained model weights to cut down immensely on training time.\n",
    "\n",
    "Here's an example of what an object detector might output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('3jD02dxL6gg', width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will create your own Duckietown object detection dataset. You will learn about the general structure such a dataset should follow. You will train an object detection model on said dataset. Finally, you will integrate said model into a ROS node, so that your duckiebot knows how to recognize duckie pedestrians (and thus avoid them).\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Setup  \n",
    "2. Investigation\n",
    "3. Data collection\n",
    "4. Training\n",
    "5. Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "### 1. Setup\n",
    "\n",
    "First, we need some global variables. These allow you to change the directory where we save of of the data you will need. You can also change the image size to reflect what your final model uses, but you can worry about that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"../dataset\"\n",
    "IMAGE_SIZE = 416\n",
    "PATH_TO_UTILS = \"/jupyter_ws/solution/utils/\"\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run(input, exception_on_failure=False):\n",
    "    try:\n",
    "        program_output = subprocess.check_output(f\"{input}\", shell=True, universal_newlines=True, stderr=subprocess.STDOUT)\n",
    "    except Exception as e:\n",
    "        if exception_on_failure:\n",
    "            raise e\n",
    "        program_output = e.output\n",
    "\n",
    "    return program_output\n",
    "\n",
    "def runp(input, exception_on_failure=False):\n",
    "    print(run(input, exception_on_failure))\n",
    "    \n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def makedirs(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except:\n",
    "        pass\n",
    "    yield None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "While you will build your own dataset with simulated images in part 2, it would be unreasonable to ask you to build your own dataset of real images. Run the cell bellow to download a dataset of labelled real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--2021-05-21 03:27:54--  https://www.dropbox.com/s/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.11.18, 2620:100:6050:18::a27d:b12\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.11.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip [following]\n",
      "--2021-05-21 03:27:54--  https://www.dropbox.com/s/raw/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com/cd/0/inline/BO7ekhZ7xF8dO9YVge2BbyCmiInkp5J_ROoLvFklZQ45thLc7_uC8QiBDZDGtI29M6JGBRDmvMr2uJ_TwgE2TOA5xEDx65ViDLrp4SzD5y4Pv6o4xokg5eV7G_g5mnFnmVety8L0XzyAlWLeyA1pPW8d/file# [following]\n",
      "--2021-05-21 03:27:55--  https://uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com/cd/0/inline/BO7ekhZ7xF8dO9YVge2BbyCmiInkp5J_ROoLvFklZQ45thLc7_uC8QiBDZDGtI29M6JGBRDmvMr2uJ_TwgE2TOA5xEDx65ViDLrp4SzD5y4Pv6o4xokg5eV7G_g5mnFnmVety8L0XzyAlWLeyA1pPW8d/file\n",
      "Resolving uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com (uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com)... 162.125.11.15, 2620:100:6050:15::a27d:b0f\n",
      "Connecting to uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com (uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com)|162.125.11.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BO4q_cDIB3FL_EhLzZ7tBIYCl9Tzshukv0u8u9ok0GtLEF02_aB_9Nk4uz0rRaALvBomD9WxK_Fp18mRE-xeerttWpV3jj-RVrDftLCu3QGyQIpiQzF_a_e9dbDoKaPNGEcApxRtl769-5HCMYN5FdjtEusM-QWcY9qcYmHWqDu2mMRux1-xCxg2iSDi7QKE6FbNdLPbAQk2mrNDEhJCpyfnfO-SHUquFbpB2jUwGIh5p1vECv8h_671xt3SpVREozXTafhHllEbxY0pypr-PcQ7kgzfeXI91tnZRazURVoeZUpGqpqNABXaIhFPaeuueOVKcWmCp2xjcfDTntMypZ4atG_Rf_7wwuREFE5Noy3kYds1brC-xdqrZnBvvf6_mn8/file [following]\n",
      "--2021-05-21 03:27:55--  https://uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com/cd/0/inline2/BO4q_cDIB3FL_EhLzZ7tBIYCl9Tzshukv0u8u9ok0GtLEF02_aB_9Nk4uz0rRaALvBomD9WxK_Fp18mRE-xeerttWpV3jj-RVrDftLCu3QGyQIpiQzF_a_e9dbDoKaPNGEcApxRtl769-5HCMYN5FdjtEusM-QWcY9qcYmHWqDu2mMRux1-xCxg2iSDi7QKE6FbNdLPbAQk2mrNDEhJCpyfnfO-SHUquFbpB2jUwGIh5p1vECv8h_671xt3SpVREozXTafhHllEbxY0pypr-PcQ7kgzfeXI91tnZRazURVoeZUpGqpqNABXaIhFPaeuueOVKcWmCp2xjcfDTntMypZ4atG_Rf_7wwuREFE5Noy3kYds1brC-xdqrZnBvvf6_mn8/file\n",
      "Reusing existing connection to uc9a33147ae4be5d6b43508dd746.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 741254387 (707M) [application/zip]\n",
      "Saving to: ‘duckietown_object_detection_dataset.zip’\n",
      "\n",
      "duckietown_object_d 100%[===================>] 706.92M  13.4MB/s    in 61s     \n",
      "\n",
      "2021-05-21 03:28:57 (11.6 MB/s) - ‘duckietown_object_detection_dataset.zip’ saved [741254387/741254387]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runp(f\"rm -rf {DATASET_DIR}\")\n",
    "runp(f\"mkdir {DATASET_DIR}\")\n",
    "# use <!> to have a download indicator\n",
    "!wget -O duckietown_object_detection_dataset.zip https://www.dropbox.com/s/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip?dl=0\n",
    "runp(f\"unzip -q duckietown_object_detection_dataset.zip -d {DATASET_DIR}\")\n",
    "runp(f\"mv {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset/* {DATASET_DIR} && rm -rf {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These real images are not the right size. Run the cell bellow to resize them (and resize the associated bounding boxes accordingly).\n",
    "\n",
    "TODO: move to utils.py and mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1963/1963 [00:14<00:00, 134.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(f\"{DATASET_DIR}/annotation/final_anns.json\") as anns:\n",
    "    annotations = json.load(anns)\n",
    "\n",
    "os.makedirs(f\"{DATASET_DIR}/images/\")\n",
    "os.makedirs(f\"{DATASET_DIR}/labels/\")\n",
    "\n",
    "npz_index = 0\n",
    "while os.path.exists(f\"{DATASET_DIR}/{npz_index}.npz\"):\n",
    "    npz_index += 1\n",
    "\n",
    "def save_img(img, boxes, classes):\n",
    "    global npz_index\n",
    "    cv2.imwrite(f\"{DATASET_DIR}/images/real_{npz_index}.jpg\", img)\n",
    "    with open(f\"{DATASET_DIR}/labels/real_{npz_index}.txt\", \"w\") as f:\n",
    "        for i in range(len(boxes)):\n",
    "            f.write(f\"{classes[i]} \"+\" \".join(map(str,boxes[i]))+\"\\n\")\n",
    "    npz_index += 1\n",
    "\n",
    "for filename in tqdm(os.listdir(f\"{DATASET_DIR}/frames\")):\n",
    "    img = cv2.imread(f\"{DATASET_DIR}/frames/{filename}\")\n",
    "\n",
    "    orig_y, orig_x = img.shape[0], img.shape[1]\n",
    "    scale_y, scale_x = IMAGE_SIZE/orig_y, IMAGE_SIZE/orig_x\n",
    "\n",
    "    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    if filename not in annotations:\n",
    "        continue\n",
    "\n",
    "    for detection in annotations[filename]:\n",
    "        box = detection[\"bbox\"]\n",
    "        label = detection[\"cat_name\"]\n",
    "\n",
    "        if label not in [\"duckie\", \"cone\"]:\n",
    "            continue\n",
    "\n",
    "        orig_x_min, orig_y_min, orig_w, orig_h = box\n",
    "\n",
    "        x_min = int(np.round(orig_x_min * scale_x))\n",
    "        y_min = int(np.round(orig_y_min * scale_y))\n",
    "        x_max = x_min + int(np.round(orig_w * scale_x))\n",
    "        y_max = y_min + int(np.round(orig_h * scale_y))\n",
    "\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        classes.append(1 if label == \"duckie\" else 2)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    #make boxes to xywh format:\n",
    "    def xminyminxmaxymax2xywfnormalized(box, image_size):\n",
    "        xmin, ymin, xmax, ymax = np.array(box, dtype=np.float64)\n",
    "        center_x = (xmin+xmax)/2\n",
    "        center_y = (ymin+ymax)/2\n",
    "        width = xmax-xmin\n",
    "        height = ymax-ymin\n",
    "\n",
    "        normalized = np.array([center_x, center_y, width, height])/image_size\n",
    "        return np.round(normalized, 5)\n",
    "    boxes = np.array([xminyminxmaxymax2xywfnormalized(box, IMAGE_SIZE) for box in boxes])\n",
    "    classes = np.array(classes)-1\n",
    "    \n",
    "    save_img(img, boxes, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, you're all set! We'll explain what all the code above was for later in this notebook.\n",
    "\n",
    "### Investigation\n",
    "\n",
    "What does an object detection dataset look like? Clearly, the specifics will depend on the convention used by specific models, but the general idea is intuitive:\n",
    "\n",
    "- We need an image\n",
    "- This image might have many bounding boxes in it, so we need some sort of list of coordinates\n",
    "- These bounding boxes must be associated with a class\n",
    "\n",
    "How are the bounding boxes defined?\n",
    "\n",
    "![image of a bounding box](../images/bbox.png)\n",
    "\n",
    "\\[Note: if you are not colorblind, you may ignore the scribbles under the colored indications for widths and heights\\]\n",
    "\n",
    "Some conventions use `x_min y_min width height`, whereas others use `x_min y_min x_max y_max`, and others use `x_center y_center width height`. In this exercise, the model we recommend ([YoloV5](https://github.com/Velythyl/yolov5)) uses `x_center y_center width height`.\n",
    "\n",
    "And how do we actually obtain these bounding boxes? In real-life applications, you would need to label a dataset of images by hand. But if you have access to a simulator that is able to segment images, you could obtain the bounding boxes directly from the segmented images. \n",
    "\n",
    "If you take a look at Pytorch's object detection [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), that is similar to what they do. While their images were segmented by hand, the tutorial uses the same technique that we will use here to obtain the bounding boxes. Their images look like this:\n",
    "\n",
    "![image with bounding boxes](../images/FudanPed.png)\n",
    "<p align=\"center\">\\[Source: https://www.cis.upenn.edu/~jshi/ped_html/\\]</p>\n",
    "\n",
    "And they simply calculate the min and max x and y coordinates of the segmented objects to obtain the bounding box.\n",
    "\n",
    "We will use the segmented mode in the Duckietown simulator to compute the bounding boxes of non-segmented images.\n",
    "\n",
    "#### What we want to detect\n",
    "\n",
    "The goal of this exercise is to make Duckietown safer: we want to be able to detect duckie pedestrians on the road and avoid squishing them. We also want to detect trucks, buses, and cones. Here is the complete list, along with their corresponding IDs:\n",
    "\n",
    "0. Duckie\n",
    "1. Cone\n",
    "2. Truck\n",
    "3. Bus\n",
    "\n",
    "\n",
    "### 3. Data collection\n",
    "\n",
    "\n",
    "#### Format\n",
    "\n",
    "Edit the [../utils/data_collection.py](../utils/data_collection.py) script so that running it produces a dataset of normal simulator images and their annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the script in the novnc browser. **NB: if this doesn't work you should rerun `dts exercises lab` with the `--vnc` flag**. \n",
    "\n",
    "Navigate to [http://localhost:8087](http://localhost:8087) and click the \"Data Collection\" icon on the desktop. This will run your [../utils/data_collection.py](../utils/data_collection.py) script. If you edit the functions below, you simply need to rerun `dts exercises build` and then re-click on the data collection. The functions are pulled from this notebook and imported into the `data_collection.py` script. \n",
    "\n",
    "The purpose of that `data_collection.py` is to automatically generate data for you from the simulator. In the rest of this exercise we will walk step by step through the process. \n",
    "\n",
    "Of course, your dataset's format depends heavily on your model. If you want to use the [YoloV5](https://github.com/duckietown/yolov5) model that we suggest, you should colosely follow their [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data).\n",
    "\n",
    "To simplify your task, we have already written a `duckietown.yaml` file (TODO where is it?) that will enable YoloV5 to understand your dataset. You will need to save your images and their bounding box data in this very specific way:\n",
    "\n",
    "![image of dataset save format](../images/dataset_format.png)\n",
    "\n",
    "In the parent directory (TODO: parent directory of what?) (named `duckietown_dataset`), you must place two subdirectories: `train` and `val` (for now, ignore `val`, or go read \"Combining with the real dataset & Validation\"). Inside `train` and `val`, there must again be two subdirectories `images` and `labels`. Inside `images`, you must place your images, and inside `labels`, you must place the images' bounding box data. Notice that the label files use the same name as their corresponding image files but with a different extension. In other words, the data for `0.jpg` can be found in `0.txt`.\n",
    "\n",
    "The format for the label files is fairly simple. For each bounding box in the corresponding image, write a row of the form `class x_center y_center width height`. Keep in mind that the pixel data must be 0-to-1 normalized (i.e., you can calculate the usual `x_center y_center width height` in pixel space and divide by the image's size). For example,\n",
    "\n",
    "    0 0.5 0.5 0.2 0.2\n",
    "    1 0.60 0.70 0.4 0.2\n",
    "\n",
    "this file says \"there is a duckie (class 0) centered in the image, whose width and height are 20% of the image's. There is also a cone whose center is at 60% of the image's maximal x value and 70% of the image's maximal y value, and its width is 40% of the image's own while its height is 20%.\"\n",
    "\n",
    "It is recommended that you read the guide posted on YoloV5's GitHub: [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating data\n",
    "\n",
    "1. Take the segmented image (this is provided to you by the simulator's rendering engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "mapping = {\n",
    "    \"house\": \"3deb34\",\n",
    "    \"bus\": \"ebd334\",\n",
    "    \"truck\": \"961fad\",\n",
    "    \"duckie\": \"cfa923\",\n",
    "    \"cone\": \"ffa600\",\n",
    "    \"floor\": \"000000\",\n",
    "    \"grass\": \"000000\",\n",
    "    \"barrier\": \"000099\"\n",
    "}\n",
    "mapping = {\n",
    "    key:\n",
    "        [int(h[i:i+2], 16) for i in (0,2,4)]\n",
    "    for key, h in mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with a few other files in the images folder. All of the original/segmented pairs are labeled as *_not_seg and *_seg\n",
    "obs = np.asarray(Image.open('../images/duckie_not_seg.png'))\n",
    "obs_seg = np.asarray(Image.open('../images/duckie_seg.png'))\n",
    "# define the mapping from objects to colours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For each color in the interesting colors (so the colors for duckies, trucks, busses, and cones):\n",
    "   \n",
    "   1. Remove all other colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def segmented_image_one_class(segmented_img, class_name):\n",
    "    mask = np.all(segmented_img == mapping[class_name], axis=-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckie_masked_image = segmented_image_one_class(np.asarray(obs_seg),\"duckie\")\n",
    "plt.imshow(duckie_masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    B. Then take that image and use it to find bounding boxes around each unique instance within the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def find_all_bboxes(mask):\n",
    "    gray = mask.astype(\"uint8\")\n",
    "    gray[mask == True] = 255\n",
    "    gray[mask == False] = 0\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\n",
    "    boxes = []\n",
    "    for index, cnt in enumerate(contours):\n",
    "        if hierarchy[0,index,3] != -1:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        boxes.append([x,y,w+x,h+y])\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes(img, boxes):\n",
    "    import matplotlib.patches as patches\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box[0], box[2]), box[1]-box[0], box[3]-box[2], linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = find_all_bboxes(duckie_masked_image)\n",
    "show_image_with_boxes(obs,boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    C. Now we want a function that does the above but for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def find_all_boxes_and_classes(segmented_img):\n",
    "\n",
    "    classes = [\"duckie\", \"cone\", \"truck\", \"bus\"]\n",
    "    all_boxes = []\n",
    "    all_classes = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        mask = segmented_image_one_class(segmented_img, class_name)\n",
    "        boxes = find_all_bboxes(mask)\n",
    "        all_boxes.extend(list(boxes))\n",
    "        classes = np.array([i]*boxes.shape[0])\n",
    "        all_classes.extend(list(classes))\n",
    "\n",
    "    return all_boxes, all_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes, all_classes = find_all_boxes_and_classes(obs_seg)\n",
    "show_image_with_boxes(obs, all_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will need to save the non-segmented version of the image, and write its bounding boxes + their classes to a corresponding txt file. You can see how this is done in [../utils/data_collection.py](../utils/data_collection.py)\n",
    "\n",
    "\n",
    "#### Combining with the real dataset & training/test set splits\n",
    "\n",
    "Notice that the images that you generated here all come from the simulator. These images are only useful in simulation: real duckies, trucks, busses, and cones look fairly different! Luckily, during Step 1 of this exercise, you downloaded a dataset of real annotated images. \n",
    "\n",
    "You should be able to view them locally in the `real_dataset` directory (TODO where is this?).\n",
    "\n",
    "You must now combine the real dataset with your dataset (the \"simulated dataset\"). But there is one extra worry: testing.\n",
    "\n",
    "When training supervised learning models, one must worry about overfitting to your training set. You should always keep *some* of your dataset *out* of your training data. This way, you can verify that your model does not overfit to your dataset by *testing* it on the data you left out. \n",
    "\n",
    "During this step, while integrating the real dataset, you should worry about the testing dataset. Perhaps you should split the real dataset (80% for training data, 20% for testing? 70/30? 50/50?), and generate new simulated images for validation too!\n",
    "\n",
    "## Finishing up\n",
    "\n",
    "Make sure if you modified these functions or the code in [../utils/data_collection.py](../data_collection.py) that you go back to the [novnc browser](http://localhost:8087) and re-run the data generation procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}