{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb39c745-56f9-495f-a3da-646610faddb5",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right\">\n",
    "  <img src=\"../images/dtlogo.png\" alt=\"Logo\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caace6d9-9d3c-4719-bace-bbc8d918f5f2",
   "metadata": {},
   "source": [
    "# üíªüöô 01 - Visual Lane Following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610d597-5e7a-4a4a-8fd1-0686debfc5ef",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/visual_control/pic1_rect.png\", width=400px>\n",
    "  <figcaption>The view from a Duckiebot when centered in its lane.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "We have explored different image processing algorithms as a means of enhancing certain aspects of an image (e.g., edges), while diminishing the presence of others (e.g., noise, irrelevant texture, etc.). We have implemented some of these tools to perform a Braitenberg-like approach to visual servoing, whereby we use weightings over edge detections to control the Duckiebot's steering in an effort to keep it in its lane as it drives forward.\n",
    "\n",
    "The previous approach was *state-less*---it reacted to the robot's image, but had no (explicit) representation of the state of the robot or the environment. Rather than design a controller that reasons over the raw image, we will process the image to estimate the robot's orientation (heading) relative to the lane. We can then combine this estimate for the robot's lane-relative heading with the PID controller that we developed earlier to in an effort to keep the robot in its lane.\n",
    "\n",
    "In the following, we will investigate one approach to estimating the robot's lane-relative orientation from images. Similar to the previous exercise, this approach makes use of the dashed-yellow and solid-white lane markings as valuable cues for determining the robot's orientation relative to the lane. The key is to isolate these lane markings from the rest of the image and then approximate them as a set of line segments. We can then transform these line segments into the ground reference frame and then determine the best-fit to their orientations relative to the robot. We can then estimate the orientation of the robot relative to the lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a416f62-ece0-4e48-ba0c-26a805d5f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to import relevant modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa0b39-de38-4322-b4c5-adde34e1146e",
   "metadata": {},
   "source": [
    "Let's load the image and the corresponding homography into Python and then generate HSV and grayscale versions for use later. \n",
    "\n",
    "**Note**: The homography below is from the robot that captured these images. For the exercises that you run in simulation and on a physical robot, the robot will use its own homography (the one that you calibrated in the case of the physical robot).\n",
    "\n",
    "**Note**: We provide several other images that you can experiment with as well, including one from the Duckiebot entering a turn. As you explore this exercise, we encourage you to experiment with some of these other images by uncommenting them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e92632-c49b-4f96-be4e-e0a28e50a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the image and generate hsv and grayscale versions\n",
    "image = cv2.imread('../images/visual_control/pic11.png')\n",
    "\n",
    "# Feel free to also experiment with some of these other images\n",
    "#image = cv2.imread('../images/visual_control/turn.png')\n",
    "#image = cv2.imread('../images/visual_control/pic10.png')\n",
    "#image = cv2.imread('../images/visual_control/pic11.png')\n",
    "#image = cv2.imread('../images/visual_control/pic12.png')\n",
    "#image = cv2.imread('../images/visual_control/pic1_rect.png')\n",
    "#image = cv2.imread('../images/visual_control/pic2_rect.png')\n",
    "#image = cv2.imread('../images/visual_control/pic3_rect.png')\n",
    "\n",
    "# OpenCV uses BGR by default, whereas matplotlib uses RGB, so we generate an RGB version for the sake of visualization\n",
    "imgrgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the image to HSV for any color-based filtering\n",
    "imghsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Most of our operations will be performed on the grayscale version\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# The image-to-ground homography associated with this image\n",
    "H = np.array([-4.137917960301845e-05, -0.00011445854191468058, -0.1595567007347241, \n",
    "              0.0008382870319844166, -4.141689222457687e-05, -0.2518201638170328, \n",
    "              -0.00023561657746150284, -0.005370140574116084, 0.9999999999999999])\n",
    "\n",
    "H = np.reshape(H,(3, 3))\n",
    "Hinv = np.linalg.inv(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfc414-9432-43dd-a600-e879381ed299",
   "metadata": {},
   "source": [
    "## Detecting Lane Markings using the Canny Edge Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7e192-5907-4d61-9ec0-2245bef51005",
   "metadata": {},
   "source": [
    "Lane markings appear as locations in the image marked by rapid changes in intensity. We have explored different approaches to isolating these *edges* based on intensity gradients, most recently using the Sobel Operator. A popular approach to detecting edges is the *Canny edge detector*. \n",
    "\n",
    "Like other edge detectors, the Canny edge detector looks for regions of the image with strong intensity gradients. The algorithm then refines these detections via non-maxima supression (a variant of which we used for the Sobel Operator-based edge detection). While non-maxima suppression is often effective at removing edges that result from noise, it can also get rid of valid edges. Thus, the Canny edge detector employs a form of hysteresis, whereby it preserves \"weak\" edges that are connected to \"strong\" edges. Here, the idea is that valid edges that may appear as weak will be connected to strong edges, whereas those that occur as a result of noise will not be connected to strong edges.\n",
    "\n",
    "In this way, the Canny edge detector divides edges into three groups:\n",
    "\n",
    "1. Those whose gradient magnitude is sufficiently large that they are identified as valid edges.\n",
    "\n",
    "2. Those whose gradient magnitude is sufficiently low that they are identified as erroneous edges.\n",
    "\n",
    "3. Those whose gradient magnitude is inconclusive and are evaluated in terms of hysteresis.\n",
    "\n",
    "The Canny edge detector differentiates between the three using two thresholds, a `canny_lower_threshold` below which edges are labeled as being erroneous, a `canny_upper_threshold` above which edges are labeled as being valid. Intensity gradients between the two thresholds are further evaluated in terms of hysteresis.\n",
    "\n",
    "We will be discussing Canny edge detection in more detail later in the course, but if you are interested in more information on how the algorithm works, please see the [Canny edge detector Wikipedia page](https://en.wikipedia.org/wiki/Canny_edge_detector).\n",
    "\n",
    "### Example: Canny Edge Detection\n",
    "\n",
    "Like any edge detector, the Canny edge detector looks for regions of the image where there are large changes in intensity. We can reduce the extent to which noise in the image leads to candidate edges by smoothing the image with a Gaussian blurring kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949020e-777c-414a-95da-3c3cc04bef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify a setting for the standard deviation that removes noise while not eliminating too much valid content.\n",
    "sigma = 2\n",
    "\n",
    "# Smooth the image using a Gaussian kernel\n",
    "img_gaussian_filter = cv2.GaussianBlur(img,(0,0), sigma)\n",
    "\n",
    "canny_lower_threshold = 80\n",
    "canny_upper_threshold = 200\n",
    "canny_kernel_size = 3;\n",
    "\n",
    "edges = cv2.Canny(img_gaussian_filter, canny_lower_threshold, canny_upper_threshold, canny_kernel_size)\n",
    "mask_edges = (edges != 0)\n",
    "dst = image * (mask_edges[:,:,None].astype(image.dtype))\n",
    "\n",
    "# Visualize the result of Canny edge detection\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original'), ax1.set_xticks([]), ax1.set_yticks([])\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.imshow(img_gaussian_filter,cmap = 'gray')\n",
    "ax2.set_title('Gaussian Filter (Sigma = ' + str(sigma) +')'), ax2.set_xticks([]), ax2.set_yticks([]);\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.imshow(cv2.cvtColor(dst, cv2.COLOR_BGR2RGB))\n",
    "ax3.set_title('Canny Edges'), ax3.set_xticks([]), ax3.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61ad33-4805-4c01-af53-da7fdabcd36d",
   "metadata": {},
   "source": [
    "## Finding the Horizon\n",
    "\n",
    "In Duckietown, the ground is planar and in other self-driving and robotics domains, it is often assumed that the ground is locally planar. We can exploit this as we search for lane markings by not searching above the horizon. In the world frame relative to which we estimated the homography (i.e., a reference frame with the origin centered between the drive wheels with the positive $x$-axis pointing forward and the positive $y$-axis to the left), the horizon corresponds to large $x$-coordinates. In Duckietown, we are only interested in the road a few meters in front of the Duckiebot.\n",
    "\n",
    "### Example: Masking out the Horizon\n",
    "\n",
    "Implement a mask that separates the ground plane from the horizon based on the homography (technically, $H^{-1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f459d04-5254-4954-8693-7ec40dda2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement mask\n",
    "\n",
    "xmax = 2\n",
    "X = np.array([xmax, 0.0, 1.0])\n",
    "x = Hinv.dot(X)\n",
    "x = x/x[-1]\n",
    "\n",
    "height = img.shape[1]\n",
    "mask_ground = np.ones(img.shape, dtype=np.uint8)\n",
    "mask_ground[0:int(np.floor(x[1])),:] = 0\n",
    "\n",
    "fig = plt.figure(figsize = (30,20))\n",
    "ax1 = fig.add_subplot(1,4,1)\n",
    "ax1.imshow(img*mask_ground,cmap = 'gray')\n",
    "ax1.set_title('Horizon Mask'), ax1.set_xticks([]), ax1.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3c822-875c-4200-a3c8-6dccccd23749",
   "metadata": {},
   "source": [
    "## Color-based Masking\n",
    "\n",
    "Having identified a candidate set of edges, we can now try design a set of masks that isolate the edges associated with the dashed-yellow and solid-white lane markings.\n",
    "\n",
    "Perhaps the most obvious thing to do is to create masks that filter out pixels whose color differs from that of the yellow and white lines. \n",
    "\n",
    "### Exercise: Color-based Masking\n",
    "\n",
    "Following what we did in the Braitenberg exercise, we can select upper- and lower-bounds on the HSV values for the two lines. One option mentioned in the Braitenberg exercise is to use [this online color picker](https://pinetools.com/image-color-picker), which allows us to get the HSV values for particular pixels. \n",
    "\n",
    "**Note**: When we go to use these bounds, remember that OpenCV uses uses the convention that $\\textrm{H} \\in [0, 179]$, $\\textrm{S} \\in [0, 255]$, and $\\textrm{V} \\in [0, 255]$, while other tools may use different ranges (e.g., Gimp uses $\\textrm{H} \\in [0, 360]$, $\\textrm{S} \\in [0, 100]$, and $\\textrm{V} \\in [0, 100])$.\n",
    "\n",
    "**Note**: While the HSV color space provides a better representation (e.g., compared to RGB) for color-based detection, the appearance of the yellow and white lane markings will change due to variations in illumination, shading, etc. for In order to improve the generalizability of these bounds, you are encouraged to also consider the `../images/visual_control/pic3_rect.png` image and define bounds that are appropriate for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128e9bf-d4ee-49e3-ac92-e314649b5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the above tool, we can identify the bounds as follows\n",
    "# Since OpenCV uses a Hue range of [0, 179], we need to divide the Hue values above by 2\n",
    "white_lower_hsv = np.array([0/2, 3*255/100, 52*255/100])\n",
    "white_upper_hsv = np.array([360/2, 26*255/100, 96*255/100])\n",
    "yellow_lower_hsv = np.array([44/2, 47*255/100, 48*255/100])\n",
    "yellow_upper_hsv = np.array([55/2, 100*255/100, 85*255/100])\n",
    "\n",
    "mask_white_orig = cv2.inRange(imghsv, white_lower_hsv, white_upper_hsv)\n",
    "mask_yellow_orig = cv2.inRange(imghsv, yellow_lower_hsv, yellow_upper_hsv)\n",
    "\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.imshow(imgrgb)\n",
    "ax1.set_title('Original'), ax1.set_xticks([]), ax1.set_yticks([])\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.imshow(imgrgb)\n",
    "ax2.imshow(mask_white_orig, cmap='jet', alpha=0.5)\n",
    "ax2.set_title('Filtered by Color (White)'), ax2.set_xticks([]), ax2.set_yticks([]);\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.imshow(imgrgb)\n",
    "ax3.imshow(mask_yellow_orig, cmap='jet', alpha=0.5)\n",
    "ax3.set_title('Filtered by Color (Yellow)'), ax3.set_xticks([]), ax3.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f39e42-bb73-454b-b311-8e2a9cb88bb8",
   "metadata": {},
   "source": [
    "### Exercise: Image dilation\n",
    "\n",
    "Conservative values for the lower- and upper-bounds used for HSV filtering, will result in many false negatives (i.e., missing pixels that actually correspond to the dashed-yellow or solid-white lane markings, but that fall outside the bounds). We can increase the range in order to capture many of these pixels, but that will generally come at the expense of false positives.\n",
    "\n",
    "One way to deal with this is to use more conservative values for the lower- and upper-bounds, but to then grow the resulting masks to capture nearby pixels that were not detected. We can do this through image *dilation*, a morphological process that expands non-zero regions in a binary image. In this case, the binary images are the HSV-based masks identified above and the non-zero regions are pixels identified as being yellow or white.\n",
    "\n",
    "In this exercise, you will experiment with the size of the dilation kernel. As you do, remember that we will be masking out the horizon, so don't worry too much if dilation results in false positives in the upper part of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb390cc8-4a2c-45fa-97e2-f3f859606154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with the size of the dilation kernel to see its effect on the masks\n",
    "dilation_kernel = np.ones((11, 11), np.uint8)\n",
    "\n",
    "mask_yellow = cv2.dilate(mask_yellow_orig, dilation_kernel, iterations=1)\n",
    "mask_white = cv2.dilate(mask_white_orig, dilation_kernel, iterations=1)\n",
    "\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(imgrgb)\n",
    "ax1.imshow(mask_yellow, cmap='jet', alpha=0.6)\n",
    "ax1.set_title('Filtered by Color (Yellow)'), ax1.set_xticks([]), ax1.set_yticks([]);\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(imgrgb)\n",
    "ax2.imshow(mask_white, cmap='jet', alpha=0.6)\n",
    "ax2.set_title('Filtered by Color (White)'), ax2.set_xticks([]), ax2.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00763323-a89c-467c-a915-ed1d07736d67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Masking\n",
    "\n",
    "Let's now combine these masks to best isolate the features that we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40a426-cffe-4b94-8ea6-b0de8f65dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_left_edge = mask_ground * mask_yellow\n",
    "mask_right_edge = mask_ground * mask_white"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2fd3d-bcaf-4a78-9613-62095eecf3e0",
   "metadata": {},
   "source": [
    "## Hough Transform\n",
    "\n",
    "At this point, we have refined the candidate set of edges corresponding to the left and right lane markings. The next step in our approach is to find a set of line segments that best fit these detections. This is problem is challenging since there will most often be multiple line segments formed by the edges and these line segments will not be continuous due to missed edge detections. \n",
    "\n",
    "\n",
    "\n",
    "An effective approach to this problem is to use the *Hough transform*, which involves a voting strategy whereby points vote on their corresponding lines. The lines receiving the most votes are identified as best fitting the given set of points.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/visual_control/hough-transform-line-parameterization.png\", width=300px>\n",
    "  <figcaption>Lines are parameterized in terms of the distance $\\rho$ between the origin and the closest point on the line and the angle $\\theta$ to this point.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "The Hough transform represents lines as\n",
    "\n",
    "$$\n",
    "\\rho = x \\cos \\theta + y \\sin \\theta\n",
    "$$\n",
    "\n",
    "where $\\rho$ is the distance from the origin to the closest point on the line and $\\theta$ is the orientation of the line between this point and the origin. The Hough transform discretizes the space of distances $[0 \\; \\rho_\\textrm{max}]$ for some user-selected $\\rho_\\textrm{max}$ and angles $[-\\pi \\; \\pi]$. The algorithm builds an *accumulator* matrix $A$,  where $A(i,j)$ represents the number of points that vote for the line parameterized by $(\\rho_i, \\theta_j)$. For each point $(x_k, y_k)$, the algorithm loops over the set of angles and adds increments the the bin $A(i,j)$ consistent with $\\rho = x_k \\cos \\theta_i + y_k \\theta_j$ (i.e., $\\rho_{i} \\leq \\rho < \\rho_{i+1}$) by one, representing a vote for that line. In our case, where we are searching for lines that best fit edges, these points correspond to pixels identified by the Canny edge detector.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/visual_control/hough-transform-example.png\", width=800px>\n",
    "  <figcaption>The Hough transform considers the set of lines consistent with each point in the scene. In this case, the points support a line with $\\theta = 60$ degrees and $\\rho \\approx 407$. (Courtesy: Wikipedia)</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "The figure above visualizes this strategy for a set of three points, where each point votes for the $(\\rho_i, \\theta_j)$ values that correspond to lines that pass through the point. The result is the line with $\\theta = 60$ degrees and $\\rho \\approx 407$ receiving the most votes.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/visual_control/hough-transform-accumulator.png\", width=400px>\n",
    "  <figcaption>A visualization of the accumulator matrix for a set of candidate distances $\\rho$ and orientations $\\theta$. Darker pixels denote $(\\rho_i, \\theta_j)$ values with few votes, while whte pixels represent $(\\rho_i, \\theta_j)$ values with a large number of votes.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "The resulting accumulator matrix will have large entries for $(\\rho_i, \\theta_j)$-parameterized lines that are consistent with many points as visualized above. The next step is to identify these peaks, which we can do using a threshold on the minimum number of votes necessary for a line to be considered valid. This will often result in the identification of multiple lines, which we would expect in the context of edges detected in images. Many implementations convert each of these lines into a line segment based upon the set of points that are consistent with (i.e., that voted for) the line.\n",
    "\n",
    "There are variations on the Hough transform that make the search more sophisticated. Among them, the one that we use below allows you to set an upper-bound on how far apart points can be while still considering to be part of the same line, as well as setting a lower-bound on the length of each line segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca68cf9-8a28-4268-85f1-00e84932a23e",
   "metadata": {},
   "source": [
    "### Example: Hough Transform\n",
    "\n",
    "In the following example, you will experiment with different parameter settings for the OpenCV implementation of the Hough transform, which include\n",
    "* `hough_threshold`: The minimum number of votes necessary for a line segment to be considered valid;\n",
    "* `hough_min_line_length`: The minimum length of a line segment in order to be considered valid;\n",
    "* `hough_max_line_gap`: The maximum spacing between points that are considered part of the same line segment;\n",
    "* `hough_rho`: The resolution at which we discretize the space of distances $\\rho$;\n",
    "* `hough_theta`: The resolution at which we discretize the angles $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc80b5-4de4-456f-bb21-d47c59c0ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with the Hough transform parameters for the left and right lane markings\n",
    "hough_threshold_left = 50\n",
    "hough_min_line_length_left = 10\n",
    "hough_max_line_gap_left = 50\n",
    "hough_rho_left = 1\n",
    "hough_theta_left = 1*np.pi/180\n",
    "\n",
    "# Parameters for white line detection\n",
    "hough_threshold_right = 50 #100\n",
    "hough_min_line_length_right = 2 #10\n",
    "hough_max_line_gap_right = 15\n",
    "hough_rho_right = 1\n",
    "hough_theta_right = 1 * np.pi/180\n",
    "\n",
    "lines_left = cv2.HoughLinesP(edges * mask_left_edge,\n",
    "                        rho=hough_rho_left,\n",
    "                        theta=hough_theta_left,\n",
    "                        threshold=hough_threshold_left,\n",
    "                        minLineLength=hough_min_line_length_left,\n",
    "                        maxLineGap=hough_max_line_gap_left)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lines_right = cv2.HoughLinesP(edges * mask_right_edge,\n",
    "                        rho=hough_rho_right,\n",
    "                        theta=hough_theta_right,\n",
    "                        threshold=hough_threshold_right,\n",
    "                        minLineLength=hough_min_line_length_right,\n",
    "                        maxLineGap=hough_max_line_gap_right)\n",
    "\n",
    "\n",
    "lines_image_left = np.zeros_like(img)\n",
    "if lines_left is not None:\n",
    "    for line in lines_left:\n",
    "        [[x1, y1, x2, y2]] = line\n",
    "        cv2.line(lines_image_left, (x1, y1), (x2, y2), (255, 0, 0), 10)\n",
    "        \n",
    "lines_image_right = np.zeros_like(img)\n",
    "if lines_right is not None:\n",
    "    for line in lines_right:\n",
    "        [[x1, y1, x2, y2]] = line\n",
    "        cv2.line(lines_image_right, (x1, y1), (x2, y2), (255, 0, 0), 10)\n",
    "\n",
    "\n",
    "# Visualize the resulting edges\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "ax1.imshow(lines_image_left, cmap='jet', alpha=0.5)\n",
    "ax1.set_title('Yellow Lane Marking'), ax1.set_xticks([]), ax1.set_yticks([])\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "ax2.imshow(lines_image_right, cmap='jet', alpha=0.5)\n",
    "ax2.set_title('White Lane Marking'), ax2.set_xticks([]), ax2.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1e9a0-bf86-4909-9ddb-b34ab33b29a6",
   "metadata": {},
   "source": [
    "## Ground Projection\n",
    "\n",
    "Unlike the previous exercise, where we only reasoned over image-space detections, it will be useful to consider these line segments when expressed in the world (ground) reference frame. We can transform coordinates expressed in the image plane to the ground plane using the camera's $3 \\times 3$ homography matrix. Here, we will use the homography defined above, but in practice (e.g., when running in simulation or on your own Duckiebot), we would use the homography for the camera that acquired the images.\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_i = H \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is the homogeneous coordinate of a point in the image plane and $\\mathbf{X}_i$ is the homogeneous coordinate of its corresponding projection in the ground frame.\n",
    "\n",
    "### Example: Image to Ground Projection\n",
    "\n",
    "In this example, you are asked to implement a `project_image_to_ground` function that takes as input the homography matrix H and a point in the image, and outputs the corresponding coordinates in the world frame. We will then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76ecbc-99f1-45ac-9c41-49a52a32ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_image_to_ground(H, x):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            H: The 3x3 image-to-ground plane homography (numpy.ndarray)\n",
    "            x: An array of non-homogeneous image coordinates, one per column (numpy.ndarray)\n",
    "        Returns:\n",
    "            X: An array of non-homogeneous coordinates in the world (ground) frame, one per column (numpy.ndarray)\n",
    "    \"\"\"\n",
    "    \n",
    "    if x.shape[0] == 2:\n",
    "        if x.ndim == 1:\n",
    "            x = np.append(x, 1)\n",
    "        else:\n",
    "            x = np.vstack((x, np.ones((1, x.shape[1]))))\n",
    "    \n",
    "    X = H.dot(x)\n",
    "    X = X/X[2,None]\n",
    "    \n",
    "    return X[0:2,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401901f-92a6-4b62-a933-1c4c9e7224aa",
   "metadata": {},
   "source": [
    "The following code uses your `project_image_to_ground` function to render the line segments from the Hough transform. You can use this to validate your implementation. Remember, the origin of the ground frame is located at the mid-point of the Duckiebot's drive wheels (and on the ground, of course), with the $x$-axis pointing forward and the $y$-axis pointing to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c0df3-e5d6-4041-b5cc-5d36dcc75d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_ground_projections (lines_left, lines_right):\n",
    "    \"\"\"\n",
    "        Renders the ground-plane projection of the left and right line detections\n",
    "     \n",
    "        Args:\n",
    "            lines_left:  An n x 4 array of candidate lines for the right lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "            lines_right: An m x 4 array of candidate lines for the left lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize = (7,7))\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    ax1.set_title('Lines Projected on Ground Plane'), ax1.set_xticks([]), ax1.set_yticks([]);\n",
    "    if lines_left is not None:\n",
    "        # Visualize the edges projected on to the ground plane\n",
    "        for line in lines_left:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            # The ground reference frame has positive X up and positivy Y left\n",
    "            # So, for the sake of plotting we treat X as Y, and Y as -X\n",
    "            ax1.plot(-Y, X, 'y-')\n",
    "\n",
    "    if lines_right is not None:\n",
    "        # Visualize the edges projeted on to the ground plane\n",
    "        for line in lines_right:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            # The ground reference frame has positive X up and positivy Y left\n",
    "            # So, for the sake of plotting we treat X as Y, and Y as -X\n",
    "            ax1.plot(-Y, X, 'k-')\n",
    "            \n",
    "render_ground_projections(lines_left, lines_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d9c97-1f26-4bfb-8d6f-d73ce7d2ce4b",
   "metadata": {},
   "source": [
    "Having detected a candidate set of lines that correspond to the left (dashed-yellow) and right (solid-white) lane markings, we can now consider their orientation in the ground frame as an indication of the Duckiebot's orientation relative to its lane.\n",
    "\n",
    "### Example: Line Orientations in the Ground Plane\n",
    "\n",
    "In this example, you will explore the orientations of the detected lines in the ground frame. We provide the routines for computing the orientations of any detected line segments.\n",
    "\n",
    "**Note**: The function below calls the `project_image_to_ground` function that you defined above. If you run into problems with your implementation of this function, consider looking at how it is implemented in the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e671f-e195-40ab-bae7-d2040cce54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lane_marking_orientations(lines_left, lines_right):\n",
    "    \"\"\"\n",
    "        Computes the orientation of any detected lines in the ground frame\n",
    "\n",
    "        Args:\n",
    "            lines_left:  An n x 4 array of candidate lines for the right lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "            lines_right: An m x 4 array of candidate lines for the left lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "                         \n",
    "        Returns:\n",
    "            theta_left:  An n-element array of the orientations of each candidate line \n",
    "                         for the left lane markings\n",
    "            theta_right  An n-element array of the orientations of each candidate line \n",
    "                         for the right lane markings\n",
    "    \"\"\"\n",
    "    \n",
    "    thetas_left = np.array([])\n",
    "    thetas_right = np.array([])\n",
    "    if lines_left is not None:\n",
    "        for line in lines_left:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            \n",
    "            ind = np.argsort(X, axis=0)\n",
    "            X = np.take_along_axis(X, ind, axis=0)\n",
    "            Y = np.take_along_axis(Y, ind, axis=0)\n",
    "            \n",
    "            theta = np.arctan2(Y[1]-Y[0], X[1]-X[0])\n",
    "            thetas_left = np.append(thetas_left, theta)\n",
    "\n",
    "    if lines_right is not None:\n",
    "        # Visualize the edges projeted on to the ground plane\n",
    "        for line in lines_right:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            \n",
    "            ind = np.argsort(X, axis=0)\n",
    "            X = np.take_along_axis(X, ind, axis=0)\n",
    "            Y = np.take_along_axis(Y, ind, axis=0)\n",
    "\n",
    "            theta = np.arctan2(Y[1]-Y[0], X[1]-X[0])\n",
    "            thetas_right = np.append(thetas_right, theta)\n",
    "            \n",
    "    return (thetas_left, thetas_right)\n",
    "\n",
    "(thetas_left, thetas_right) = get_lane_marking_orientations(lines_left, lines_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f7499-9ce7-435b-8852-99c2e48a83da",
   "metadata": {},
   "source": [
    "Depending on the number and nature of the line segments that you identified earlier using the Hough transform, there may be a lot of variance in the orientations. We can see this by looking at the histograms for the left and right orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1ac8a-5c4a-45fa-b812-ff5727a94254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, let's apply the mask to our gradient directions\n",
    "fig = plt.figure(figsize = (10,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.hist(thetas_left*180/np.pi, bins=10)\n",
    "ax1.set_title('Gradient Direction Histogram (Left Edge)')\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.hist(thetas_right*180/np.pi, bins=10)\n",
    "ax2.set_title('Gradient Direction Histogram (Right Edge)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3794d-d449-4f03-924d-5002ec4328f5",
   "metadata": {},
   "source": [
    "If one of the histograms is too spread out and there isn't a clear or valid mode (i.e., there is no dominant peak or there is a dominant peak, but it corresponds to noise and is wrong), we will want to revisit the parameters that we used for the Hough transform to better isolate lines corresponding to the lane markings.\n",
    "\n",
    "Depending on the number of bins that we used to generate the histogram, we also may see histograms that are peaked with a large number of lines having similar orientations. This can occur when there are other edges in the image that share the same orientation, namely the solid-white lane marking to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dbf4c-fbfe-44c9-87f1-cbf5de232a20",
   "metadata": {},
   "source": [
    "## Estimating the Duckiebot's Lane-relative Orientation\n",
    "\n",
    "Assuming that we have a valid set of line segments corresponding to the lane markings, we can use their orientations to estimate the Duckiebot's orientation relative to the lane. There are different ways that we can formulate this problem depending on the amount of outliers.\n",
    "\n",
    "### Example: Using the Mode of the Distribution\n",
    "\n",
    "One approach that we can adopt is to look at the mode of the distribution over the orientations of all the candidate line segments. This has the advantage that outliers don't affect the resulting estimate, unless, of course, they are the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d95f2-1602-458c-99d3-ca98e7fbd72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thetas = np.append(thetas_left, thetas_right)\n",
    "\n",
    "# Determine the orientation of the left and right gradients as the mode of each histogram\n",
    "(hist_thetas, bins_thetas) = np.histogram(thetas, bins=30)\n",
    "\n",
    "idx = hist_thetas.argmax()\n",
    "theta_argmax = (bins_thetas[idx] + bins_thetas[idx+1])/2\n",
    "\n",
    "print('Mode: %.2f degrees' %( np.rad2deg(theta_argmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f2cac-fd41-4cda-b2e5-28f7cc1d62a2",
   "metadata": {},
   "source": [
    "### Example: Using the Mean of the Distribution\n",
    "\n",
    "If the distribution over the orientations of the line segments is spread out, as might be the case for a curve, we can consider taking the mean of the set of orientations (including both the left and right candidate lane markings). A drawback of this approach is that any erroneous line segments will affect the resulting estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b0cbb-ffc4-49cd-b96c-18bee564cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.append(thetas_left, thetas_right)\n",
    "\n",
    "theta_mean = np.mean(thetas)\n",
    "\n",
    "print(\"Mean: %.2f degrees\" %(np.rad2deg(theta_mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16757f5-2677-46e1-bca6-f13bcab16bba",
   "metadata": {},
   "source": [
    "# üíª üöô Write the PID lane-following function\n",
    "\n",
    "Now, we can consider combining the ability to estimate the orientation of the robot relative to its lane with a feedback controller that steers the vehicle (i.e., changes its angular rate) in order to maintain a desired orientation relative to the lane. In this case, an orientation of zero degrees. \n",
    "\n",
    "Here, we ask you to implement the following two functions:\n",
    "\n",
    "1. A function that takes as input the image from the Duckiebot's forward-facing camera and outputs an estimate of the robot's orientation relative to the lane.\n",
    "\n",
    "2. A PID controller that actuates the angular rate of the Duckiebot in order to maintain a dersired orientation relative to the lane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f68a9-053f-40e1-a845-061567d08345",
   "metadata": {},
   "source": [
    "## PID Heading Controller\n",
    "\n",
    "We will use a PID controller to control the Duckiebot's heading relative to its lane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c0ff3-000f-48e7-8390-4ff0ccec1dbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "‚ö†Ô∏è ***WARNING:***\n",
    "- DO NOT CHANGE THE NAME OF THE FOLLOWING FUNCTION\n",
    "- DO NOT CHANGE THE ARGUMENTS OF THE FUNCTION\n",
    "- DO NOT CREATE NEW CODE CELLS, THEY WILL NOT BE CONSIDERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b8061-1a05-457f-8180-25cc96837487",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# The function written in this cell will actually be ran on your robot (sim or real). \n",
    "# Put together the steps above and write your PIDController function! \n",
    "# DO NOT CHANGE THE NAME OF THIS FUNCTION, INPUTS OR OUTPUTS, OR THINGS WILL BREAK\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# TODO: write your PID function for heading control!\n",
    "\n",
    "def PIDController(v_0, theta_ref, theta_hat, prev_e, prev_int, delta_t):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        v_0 (:double:) linear Duckiebot speed (given).\n",
    "        theta_ref (:double:) reference heading pose\n",
    "        theta_hat (:double:) the current estiamted theta.\n",
    "        prev_e (:double:) tracking error at previous iteration.\n",
    "        prev_int (:double:) previous integral error term.\n",
    "        delta_t (:double:) time interval since last call.\n",
    "    returns:\n",
    "        v_0 (:double:) linear velocity of the Duckiebot \n",
    "        omega (:double:) angular velocity of the Duckiebot\n",
    "        e (:double:) current tracking error (automatically becomes prev_e_y at next iteration).\n",
    "        e_int (:double:) current integral error (automatically becomes prev_int_y at next iteration).\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: these are random values, you have to implement your own PID controller in here\n",
    "    omega = np.random.uniform(-8.0, 8.0)\n",
    "    e = np.random.random()\n",
    "    e_int = np.random.random()\n",
    "    \n",
    "    # SOLUTION\n",
    "    # Tracking error\n",
    "    e = theta_ref - theta_hat\n",
    "\n",
    "    # integral of the error\n",
    "    e_int = prev_int + e*delta_t\n",
    "\n",
    "    # anti-windup - preventing the integral error from growing too much\n",
    "    e_int = max(min(e_int,2),-2)\n",
    "\n",
    "    # derivative of the error\n",
    "    e_der = (e - prev_e)/delta_t\n",
    "\n",
    "    # controller coefficients\n",
    "    Kp = 5\n",
    "    Ki = 0.2\n",
    "    Kd = 0.1\n",
    "\n",
    "    # PID controller for omega\n",
    "    omega = Kp*e + Ki*e_int + Kd*e_der\n",
    "    \n",
    "    #print(f\"\\n\\nDelta time : {delta_t} \\nE : {np.rad2deg(e)} \\nE int : {e_int} \\nPrev e : {prev_e} \\nU : {u} \\nTheta hat: {np.rad2deg(theta_hat)} \\n\")\n",
    "        \n",
    "    return [v_0, omega], e, e_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4884bb-cadc-48ce-9049-27589e32c780",
   "metadata": {},
   "source": [
    "## Estimating the Robot's Lane-relative Orientation  \n",
    "\n",
    "Based on what we have learned above, write a function that takes as input the image from the Duckiebot's camera (in the BGR color space) and outputs an estimate of the robot's orienatation relative to the lane, along with the line segments associated with the left and right lane markings *in the ground reference frame* (i.e., the reference frame centered between the Duckiebot's drive wheels with the positive $x$-axis pointing forward and the positive $y$-axis pointing left. The orientation will serve as the estimate of the robot's lane-relative heading that is used by the PID controller, while we will use the line segments for visualization.\n",
    "\n",
    "You are welcome to add additional functions in the cell below that you want to call from your `estimate_lane_relative_headings` function (e.g., the `project_image_to_ground` function that you created above, or the `get_lane_marking_orientations`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135f07a-7a41-4c7d-9208-850e8c5d2bb0",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è ***WARNING:***\n",
    "- DO NOT CHANGE THE NAME OF THE FOLLOWING FUNCTION\n",
    "- DO NOT CHANGE THE ARGUMENTS OF THE FUNCTION\n",
    "- DO NOT CREATE NEW CODE CELLS, THEY WILL NOT BE CONSIDERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847717de-ee7f-4d44-8f89-a6012b4d6a65",
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# The function written in this cell will actually be ran on your robot (sim or real). \n",
    "# Put together the steps above and write your DeltaPhi function! \n",
    "# DO NOT CHANGE THE NAME OF THIS FUNCTION, INPUTS OR OUTPUTS, OR THINGS WILL BREAK\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#TODO: write a correct function\n",
    "\n",
    "def estimate_lane_relative_heading(H, image):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            H: The 3x3 image-to-ground plane homography (numpy.ndarray)\n",
    "            image: An image from the robot's camera in the BGR color space (numpy.ndarray)\n",
    "        Return:\n",
    "            theta_hat: An estimate of the robot's heading (radians) relative to the lane (float)\n",
    "            lm_left_ground:  An n x 4 array of candidate lines for the right lane markings (numpy.ndarray)\n",
    "                             Each row [x0, y0, x1, y1] specifies the line-segment coordinate in the ground frame\n",
    "            lm_right_ground: An m x 4 array of candidate lines for the left lane markings (numpy.ndarray)\n",
    "                             Each row [x0, y0, x1, y1] specifies the line-segment coordinate in the ground frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: these are random values, you have to implement your own solution in here\n",
    "    theta_hat = np.random.random()\n",
    "    lm_left_ground = np.empty((0, 4), float)\n",
    "    lm_right_ground = np.empty((0, 4), float)\n",
    "    \n",
    "    # SOLUTION\n",
    "    \n",
    "    imghsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    Hinv = np.linalg.inv(H)\n",
    "    \n",
    "    # Masking the horizon\n",
    "    xmax = 2\n",
    "    X = np.array([xmax, 0.0, 1.0])\n",
    "    x = Hinv.dot(X)\n",
    "    x = x/x[-1]\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "    mask_ground = np.ones((height, width), dtype=np.uint8)\n",
    "    mask_ground[0:int(np.floor(x[1])),:] = 0\n",
    "    \n",
    "    \n",
    "    # Gaussian smoothing\n",
    "    sigma = 2\n",
    "    img_gaussian_filter = cv2.GaussianBlur(image,(0,0), sigma)\n",
    "\n",
    "    # Canny edge detection\n",
    "    canny_lower_threshold = 80\n",
    "    canny_upper_threshold = 200\n",
    "    canny_kernel_size = 3;\n",
    "\n",
    "    edges = cv2.Canny(img_gaussian_filter, canny_lower_threshold, canny_upper_threshold, canny_kernel_size)\n",
    "\n",
    "    \n",
    "    # Color-based masking\n",
    "    # Using the above tool, we can identify the bounds as follows\n",
    "    # Since OpenCV uses a Hue range of [0, 179], we need to divide the Hue values above by 2\n",
    "    white_lower_hsv = np.array([0/2, 3*255/100, 52*255/100])\n",
    "    white_upper_hsv = np.array([360/2, 26*255/100, 96*255/100])\n",
    "    yellow_lower_hsv = np.array([44/2, 47*255/100, 48*255/100])\n",
    "    yellow_upper_hsv = np.array([55/2, 100*255/100, 85*255/100])\n",
    "\n",
    "    mask_white_orig = cv2.inRange(imghsv, white_lower_hsv, white_upper_hsv)\n",
    "    mask_yellow_orig = cv2.inRange(imghsv, yellow_lower_hsv, yellow_upper_hsv)\n",
    "\n",
    "    # Color dilation\n",
    "    dilation_kernel = np.ones((11, 11), np.uint8)\n",
    "\n",
    "    mask_yellow = cv2.dilate(mask_yellow_orig, dilation_kernel, iterations=1)\n",
    "    mask_white = cv2.dilate(mask_white_orig, dilation_kernel, iterations=1)\n",
    "    \n",
    "    # Masking\n",
    "    mask_left_edge = mask_ground * mask_yellow\n",
    "    mask_right_edge = mask_ground * mask_white\n",
    "  \n",
    "    # Hough Transform\n",
    "    # Parameters for left line detection\n",
    "    hough_threshold_left = 50\n",
    "    hough_min_line_length_left = 10\n",
    "    hough_max_line_gap_left = 50\n",
    "    hough_rho_left = 1\n",
    "    hough_theta_left = 1*np.pi/180\n",
    "\n",
    "    # Parameters for white line detection\n",
    "    hough_threshold_right = 50 #100\n",
    "    hough_min_line_length_right = 2 #10\n",
    "    hough_max_line_gap_right = 15\n",
    "    hough_rho_right = 1\n",
    "    hough_theta_right = 1 * np.pi/180\n",
    "\n",
    "    lines_left = np.array([])\n",
    "    lines_right = np.array([])\n",
    "    lines_left = cv2.HoughLinesP(edges * mask_left_edge,\n",
    "                            rho=hough_rho_left,\n",
    "                            theta=hough_theta_left,\n",
    "                            threshold=hough_threshold_left,\n",
    "                            minLineLength=hough_min_line_length_left,\n",
    "                            maxLineGap=hough_max_line_gap_left)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lines_right = cv2.HoughLinesP(edges * mask_right_edge,\n",
    "                            rho=hough_rho_right,\n",
    "                            theta=hough_theta_right,\n",
    "                            threshold=hough_threshold_right,\n",
    "                            minLineLength=hough_min_line_length_right,\n",
    "                            maxLineGap=hough_max_line_gap_right)\n",
    "    \n",
    "    (thetas_left, thetas_right) = get_lane_marking_orientations(H, lines_left, lines_right)\n",
    "    thetas = np.append(thetas_left, thetas_right)\n",
    "\n",
    "    # Determine the orientation of the left and right gradients as the mode of each histogram\n",
    "    (hist_thetas, bins_thetas) = np.histogram(thetas, bins=30)\n",
    "\n",
    "    idx = hist_thetas.argmax()\n",
    "    theta_argmax = (bins_thetas[idx] + bins_thetas[idx+1])/2\n",
    "    theta_mean = np.mean(thetas)\n",
    "\n",
    "    theta_hat = theta_argmax\n",
    "    \n",
    "    \n",
    "    # Project these to the ground frame\n",
    "    lm_left_ground = np.empty((0, 4), float)\n",
    "    lm_right_ground = np.empty((0, 4), float)\n",
    "    if lines_left is not None:\n",
    "        for line in lines_left:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1).flatten()\n",
    "            XY2 = project_image_to_ground(H, xy2).flatten()\n",
    "\n",
    "            lm_left_ground = np.append(lm_left_ground, np.array([[XY1[0], XY1[1], XY2[0], XY2[1]]]), axis=0)\n",
    "\n",
    "    if lines_right is not None:\n",
    "        for line in lines_right:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1).flatten()\n",
    "            XY2 = project_image_to_ground(H, xy2).flatten()\n",
    "            \n",
    "            lm_right_ground = np.append(lm_right_ground, np.array([[XY1[0], XY1[1], XY2[0], XY2[1]]]), axis=0)\n",
    "\n",
    "    #print(lm_right_ground)\n",
    "    return (theta_hat, lm_left_ground, lm_right_ground)\n",
    "\n",
    "\n",
    "\n",
    "def project_image_to_ground(H, x):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            H: The 3x3 image-to-ground plane homography (numpy.ndarray)\n",
    "            x: An array of non-homogeneous image coordinates, one per column (numpy.ndarray)\n",
    "        Returns:\n",
    "            X: An array of non-homogeneous coordinates in the world (ground) frame, one per column (numpy.ndarray)\n",
    "    \"\"\"\n",
    "    \n",
    "    if x.shape[0] == 2:\n",
    "        if x.ndim == 1:\n",
    "            x = np.append(x, 1)\n",
    "        else:\n",
    "            x = np.vstack((x, np.ones((1, x.shape[1]))))\n",
    "    \n",
    "    X = H.dot(x)\n",
    "    X = X/X[2,None]\n",
    "    \n",
    "    return X[0:2,]\n",
    "\n",
    "\n",
    "\n",
    "def get_lane_marking_orientations(H, lines_left, lines_right):\n",
    "    \"\"\"\n",
    "        Computes the orientation of any detected lines in the ground frame\n",
    "\n",
    "        Args:\n",
    "            H:           The 3x3 image-to-ground plane homography (numpy.ndarray)\n",
    "            lines_left:  An n x 4 array of candidate lines for the right lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "            lines_right: An m x 4 array of candidate lines for the left lane markings\n",
    "                         Each row [x0, y0, x1, y1] specifies the line-segment coordinate\n",
    "                         \n",
    "        Returns:\n",
    "            theta_left:  An n-element array of the orientations of each candidate line \n",
    "                         for the left lane markings\n",
    "            theta_right  An n-element array of the orientations of each candidate line \n",
    "                         for the right lane markings\n",
    "    \"\"\"\n",
    "    \n",
    "    thetas_left = np.array([])\n",
    "    thetas_right = np.array([])\n",
    "    if lines_left is not None:\n",
    "        for line in lines_left:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            \n",
    "            ind = np.argsort(X, axis=0)\n",
    "            X = np.take_along_axis(X, ind, axis=0)\n",
    "            Y = np.take_along_axis(Y, ind, axis=0)\n",
    "            \n",
    "            theta = np.arctan2(Y[1]-Y[0], X[1]-X[0])\n",
    "            thetas_left = np.append(thetas_left, theta)\n",
    "\n",
    "    if lines_right is not None:\n",
    "        for line in lines_right:\n",
    "            [[x1, y1, x2, y2]] = line\n",
    "            xy1 = np.array([[x1, y1]]).transpose()\n",
    "            xy2 = np.array([[x2, y2]]).transpose()\n",
    "\n",
    "            # Project to the ground frame\n",
    "            XY1 = project_image_to_ground(H, xy1)\n",
    "            XY2 = project_image_to_ground(H, xy2)\n",
    "\n",
    "            X = np.array([XY1[0], XY2[0]])\n",
    "            Y = np.array([XY1[1], XY2[1]])\n",
    "            \n",
    "            ind = np.argsort(X, axis=0)\n",
    "            X = np.take_along_axis(X, ind, axis=0)\n",
    "            Y = np.take_along_axis(Y, ind, axis=0)\n",
    "\n",
    "            theta = np.arctan2(Y[1]-Y[0], X[1]-X[0])\n",
    "            thetas_right = np.append(thetas_right, theta)\n",
    "            \n",
    "    return (thetas_left, thetas_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a018e9-e570-49ab-b1b7-cabb23cd2a69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the `estimate_lane_relative_heading` function\n",
    "\n",
    "As we have seen, unit tests are valuable in confirming that a particular piece of code works as intended when provided with an expected input.\n",
    "\n",
    "Let's see if the function you wrote above passes the following test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554f27d-eefb-45e8-a86f-8530217f2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from unit_test import UnitTestELRH\n",
    "\n",
    "# This function has hardcoded values (bad practice!) to test the LMOrientation function above. \n",
    "# The test will be successful if you get angles around 220 degrees (left edge) and 320 degrees (right edge)\n",
    "\n",
    "UnitTestELRH(estimate_lane_relative_heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd400869-aad5-46af-9196-e06eb019d082",
   "metadata": {},
   "source": [
    "## üíª Test your PID lane-following controller in the simulator\n",
    "\n",
    "1. Open a terminal on your computer, and type \n",
    "\n",
    "       dts exercises build\n",
    "\n",
    "\n",
    "2. Wait for the build to finish, then type:\n",
    "\n",
    "       dts exercises test --sim\n",
    "\n",
    "\n",
    "3. Open VNC on you browser and click on the `VLF - Visual Lane Following Exercise` icon on your desktop. You will see the following open (it might take ~30second or more, depending on the specifications of your computer):\n",
    "\n",
    "    - A preconfigured RVIZ: to visualize your detections\n",
    "    - A GUI with buttons labeled `Go` and `Stop`.\n",
    "\n",
    "\n",
    "4. Place the Duckiebot in the center of its lane oriented in the direction of travel. Click `Go` in the GUI to activate your controller. The Duckiebot will start driving. You can click `Stop` at any time to stop the Duckiebot, and resume by clicking `Go`.\n",
    "\n",
    "On the right side of the  RVIZ window, you should see an image showing the current view from the Duckiebot's camera.\n",
    "\n",
    "On the terminal on your computer where you launched the activity, you will see some some initial instructions as well as some debugging information.\n",
    "\n",
    "To test different solutions, change the `estimate_lane_relative_heading()` and `PIDController()` functions above. Each time you do, make sure to save this file (`Ctrl-S`) and then re-run the activity with `dts exercises test --sim`.\n",
    "\n",
    "**Note**: Clicking on the icon will also bring up an LX terminal, which may print out an error message that stating \"Tried to advertise a service that is already advertised in this node ...\". You can ignore this error.\n",
    "\n",
    "**Note** The lower- and upper-bounds for the color that you identified earlier may not be appropriate in the simulator. You many want to experiment with more conservative bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ddbe8-e0f2-4df3-952f-7c4c83052e2f",
   "metadata": {},
   "source": [
    "## üöô Test your PID lane-following controller on your Duckiebot\n",
    "\n",
    "1. Open a terminal on your computer, and type \n",
    "\n",
    "       dts exercises build\n",
    "\n",
    "\n",
    "2. Wait for the build to finish, then type:\n",
    "\n",
    "       dts exercises test -b ROBOTNAME\n",
    "\n",
    "\n",
    "3. Follow the same instructions at point 3 for the simulation-based evaluation above.\n",
    "\n",
    "**Note** The lower- and upper-bounds for the color that you identified earlier may not be appropriate for your Duckiebot. You many want to experiment with more conservative bounds or estimate the bounds based on images from your Duckiebot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094e01a-d9f4-46a0-bef3-ee768c2e0cbd",
   "metadata": {},
   "source": [
    "# Local evaluation and remote submission of your homework exercise\n",
    "\n",
    "‚ö†Ô∏è You must submit this homework for evaluation if you are pursuing a verified track in the MOOC, by following the `Remote evalutation` instructions below.‚ö†Ô∏è \n",
    "\n",
    "## Local evaluation\n",
    "\n",
    "If you want (this is not necessary) you can evaluate your submission locally before shipping it to the cloud. This will provide you access to detailed performacne metrics of your implementation on various episodes. Note that this will take a while to run (~30-60 minutes). \n",
    "\n",
    "1. Open a terminal, navigate to the exercise folder and run:\n",
    "\n",
    "\n",
    "        dts challenges evaluate\n",
    "        \n",
    "\n",
    "2. The result of the simulation can be visualized in realtime at the link printed by the evaluator, for example:\n",
    "\n",
    "\n",
    "3. The evaluation output is saved locally at the end of the evaluation process. \n",
    "\n",
    "## Remote submission (‚ö†Ô∏è This part is mandatory to submit your homework and receive an official evaluation ‚ö†Ô∏è)\n",
    "\n",
    "You can submit your agent for evaluation by: \n",
    "\n",
    "1. Opening a terminal on your computer, navigating to the exercise folder and running:\n",
    "\n",
    "\n",
    "        dts challenges submit\n",
    "        \n",
    "\n",
    "2. The result of the submission can be visualize on the AIDO challenges website:\n",
    "\n",
    "After some processing, you should see something like this:\n",
    "\n",
    "```\n",
    "\n",
    "~        ## Challenge mooc-viscontrol - MOOC - Visual Lane Following\n",
    "~        \n",
    "~                Track this submission at:\n",
    "~        \n",
    "~                    https://challenges.duckietown.org/v4/humans/submissions/SUBMISSION-NUMBER\n",
    "~        \n",
    "~                You can follow its fate using:\n",
    "~        \n",
    "~                    $ dts challenges follow --submission SUBMISSION-NUMBER\n",
    "~        \n",
    "~                You can speed up the evaluation using your own evaluator:\n",
    "~        \n",
    "~                    $ dts challenges evaluator --submission SUBMISSION-NUMBER\n",
    "~        \n",
    "~                For more information, see the manual at https://docs.duckietown.org/daffy/AIDO/out/\n",
    "~        \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b011ed4-e1fc-4a73-9a35-93f408df6120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
